{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f842d-2659-4914-825b-7443ddb45128",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning & Quality Assurance \n",
    "### Smart City Energy Dataset (Asian Power Standards)\n",
    "\n",
    "This notebook represents the **second step** of the Smart City Energy Analytics pipeline. \n",
    "While the previous notebook (`01_data_import_documented.ipynb`) focuses on loading the dataset, \n",
    "this notebook performs a **full data cleaning and quality assurance workflow** based on \n",
    "**Asian power grid standards**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Notebook Goals\n",
    "\n",
    "This notebook prepares the smart grid dataset for analytics and modeling by:\n",
    "\n",
    "- Checking the **overall data quality and structure**\n",
    "- Cleaning the **Country** column (removing non-country labels)\n",
    "- Handling **missing values** in a robust way\n",
    "- Removing **duplicate rows**\n",
    "- Detecting **outliers** in:\n",
    "- Voltage (V)\n",
    "- Current (A)\n",
    "- Power Consumption\n",
    "- Removing outliers and validating the final cleaned dataset\n",
    "\n",
    "The final cleaned dataset is stored in the variable **`df_final`** and is ready for:\n",
    "\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Statistical modeling\n",
    "- Machine learning / forecasting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30014d6c-3a7a-4c0b-9581-507b2a04c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ASIAN POWER GRID – CLEANING PIPELINE STARTED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ASIAN POWER GRID – CLEANING PIPELINE STARTED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637b5e3-92dd-4fe7-ae96-d2947c6ef278",
   "metadata": {},
   "source": [
    "### Step 1 – Configuration\n",
    "This step defines the input file path and the output folder for saving cleaned datasets. \n",
    "If the output folder does not already exist, it is automatically created. \n",
    "A timestamp is also generated to uniquely label exported files. \n",
    "This ensures that all cleaning operations have a consistent and organized file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47aaca4a-37d6-4b3a-90f9-8e7c9ab20ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created folder: cleaned_datasets\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "INPUT_FILE = \"C:/0_DA/Iot_DataAnalyst/smart_grid_dataset_city_modified.csv\"\n",
    "OUTPUT_FOLDER = \"cleaned_datasets\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "    print(f\"[INFO] Created folder: {OUTPUT_FOLDER}\")\n",
    "else:\n",
    "    print(f\"[INFO] Output folder exists: {OUTPUT_FOLDER}\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded998d9-210a-448e-bd8d-a6cf831c4dda",
   "metadata": {},
   "source": [
    "### Step 2 – Load Data\n",
    "This step reads the input CSV file into a DataFrame and includes error handling to confirm successful loading or report issues, while also displaying the original dataset shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69e1e6f-c497-4ace-82cf-861cec49d4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 1] LOADING DATA\n",
      "================================================================================\n",
      "[SUCCESS] Loaded file: C:/0_DA/Iot_DataAnalyst/smart_grid_dataset_city_modified.csv\n",
      "[INFO] Original shape: (50010, 27)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 1] LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"[SUCCESS] Loaded file: {INPUT_FILE}\")\n",
    "    print(f\"[INFO] Original shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] File not found: {INPUT_FILE}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not load file: {e}\")\n",
    "    raise\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03786756-0164-410a-9600-1d2bec7a07a8",
   "metadata": {},
   "source": [
    "### Step 3 – Data Quality Check\n",
    "This step examines the dataset’s structure and completeness by displaying column information and counting missing values. \n",
    "It helps verify data types, detect potential issues, and ensure the dataset is ready for further cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50aaa14-e344-4a2b-9119-fbf8458a11d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 2] DATA QUALITY CHECK\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50010 entries, 0 to 50009\n",
      "Data columns (total 27 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Timestamp                    50010 non-null  object \n",
      " 1   City                         50010 non-null  object \n",
      " 2   Name                         50010 non-null  object \n",
      " 3   Country                      50010 non-null  object \n",
      " 4   region                       50010 non-null  object \n",
      " 5   Voltage (V)                  50010 non-null  float64\n",
      " 6   Current (A)                  50010 non-null  float64\n",
      " 7   Power Consumption (kW)       50010 non-null  float64\n",
      " 8   Reactive Power (kVAR)        50010 non-null  float64\n",
      " 9   Power Factor                 50010 non-null  float64\n",
      " 10  Solar Power (kW)             50010 non-null  float64\n",
      " 11  Wind Power (kW)              50010 non-null  float64\n",
      " 12  Grid Supply (kW)             50010 non-null  float64\n",
      " 13  Voltage Fluctuation (%)      50010 non-null  float64\n",
      " 14  Overload Condition           50010 non-null  int64  \n",
      " 15  Transformer Fault            50010 non-null  int64  \n",
      " 16  Temperature (°C)             50010 non-null  float64\n",
      " 17  Humidity (%)                 50010 non-null  float64\n",
      " 18  Electricity Price (EUR/kWh)  50010 non-null  float64\n",
      " 19  Predicted Load (kW)          50010 non-null  float64\n",
      " 20  SolarIrradiance              50010 non-null  float64\n",
      " 21  WindSpeed                    50010 non-null  float64\n",
      " 22  Sensor_ID                    50010 non-null  object \n",
      " 23  SensorType                   50010 non-null  object \n",
      " 24  Manufacturer                 50010 non-null  object \n",
      " 25  InstallationDate             50010 non-null  object \n",
      " 26  DecommissionStatus           50010 non-null  object \n",
      "dtypes: float64(15), int64(2), object(10)\n",
      "memory usage: 10.3+ MB\n",
      "\n",
      "[INFO] Missing values per column:\n",
      "Timestamp                      0\n",
      "City                           0\n",
      "Name                           0\n",
      "Country                        0\n",
      "region                         0\n",
      "Voltage (V)                    0\n",
      "Current (A)                    0\n",
      "Power Consumption (kW)         0\n",
      "Reactive Power (kVAR)          0\n",
      "Power Factor                   0\n",
      "Solar Power (kW)               0\n",
      "Wind Power (kW)                0\n",
      "Grid Supply (kW)               0\n",
      "Voltage Fluctuation (%)        0\n",
      "Overload Condition             0\n",
      "Transformer Fault              0\n",
      "Temperature (°C)               0\n",
      "Humidity (%)                   0\n",
      "Electricity Price (EUR/kWh)    0\n",
      "Predicted Load (kW)            0\n",
      "SolarIrradiance                0\n",
      "WindSpeed                      0\n",
      "Sensor_ID                      0\n",
      "SensorType                     0\n",
      "Manufacturer                   0\n",
      "InstallationDate               0\n",
      "DecommissionStatus             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. DATA QUALITY CHECK\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 2] DATA QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df.info()\n",
    "print(\"\\n[INFO] Missing values per column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda672b-3d66-4c09-960c-59899d5e679a",
   "metadata": {},
   "source": [
    "### Step 4 – Handle Missing Numeric Values\n",
    "In this step, all numeric columns are checked for missing values, and any gaps are filled using the column median. \n",
    "Median imputation is used because it is robust to outliers and preserves the central tendency of each feature. \n",
    "A cleaned copy of the dataset is created, and the process ensures no numeric missing values remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6f3359-5755-4f14-91c9-889b4c2bd8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 3] HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "[DEBUG] Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. HANDLE MISSING NUMERIC VALUES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 3] HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing_count = df_cleaned[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        median_val = df_cleaned[col].median()\n",
    "        df_cleaned[col].fillna(median_val, inplace=True)\n",
    "        print(f\"[FIX] {col}: Filled {missing_count} missing values with median {median_val}\")\n",
    "\n",
    "print(f\"[DEBUG] Remaining missing values: {df_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22df029-4321-4451-9eba-2d4bd8657f68",
   "metadata": {},
   "source": [
    "### Step 5 – Remove Duplicates\n",
    "This step identifies any duplicate rows in the dataset and removes them to prevent repeated observations from biasing the analysis. \n",
    "Duplicate detection ensures data integrity, and only unique records are retained in the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14d6288-a9d9-40e5-9994-b7aa9f23bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 4] DUPLICATE REMOVAL\n",
      "================================================================================\n",
      "[INFO] Duplicate rows found: 10\n",
      "[SUCCESS] Removed 10 duplicates\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. REMOVE DUPLICATES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 4] DUPLICATE REMOVAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "duplicate_count = df_cleaned.duplicated().sum()\n",
    "print(f\"[INFO] Duplicate rows found: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"[SUCCESS] Removed {duplicate_count} duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfa759-a20f-483e-8675-e167b6444d8c",
   "metadata": {},
   "source": [
    "### Step 6 – Outlier Detection\n",
    "In this step, outliers are identified using both domain knowledge and statistical rules. \n",
    "Voltage outliers are detected based on a realistic operating range for Asian grids (90–250 V), while Current and Power Consumption outliers are found using the IQR method. \n",
    "All detected outlier indices are collected to understand how many records may require removal or special handling in later cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb86bc06-b6ce-4fb5-b2c9-e6e0214d02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 5] OUTLIER DETECTION\n",
      "================================================================================\n",
      "[OUTLIER] Voltage: 16\n",
      "[OUTLIER] Current (A): 12\n",
      "[SUMMARY] TOTAL OUTLIER ROWS: 16\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. OUTLIER DETECTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 5] OUTLIER DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ASIA_VOLTAGE_RANGE = {\"min\": 90, \"max\": 250}\n",
    "outlier_indices = set()\n",
    "\n",
    "# Voltage outliers (domain-based)\n",
    "if \"Voltage (V)\" in df_cleaned.columns:\n",
    "    v_outliers = df_cleaned[(df_cleaned[\"Voltage (V)\"] < ASIA_VOLTAGE_RANGE[\"min\"]) |\n",
    "                            (df_cleaned[\"Voltage (V)\"] > ASIA_VOLTAGE_RANGE[\"max\"])]\n",
    "    outlier_indices.update(v_outliers.index)\n",
    "    print(f\"[OUTLIER] Voltage: {len(v_outliers)}\")\n",
    "\n",
    "# IQR function\n",
    "def detect_outliers_iqr(data, col):\n",
    "    Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return data[(data[col] < lower) | (data[col] > upper)]\n",
    "\n",
    "# Current outliers\n",
    "if \"Current (A)\" in df_cleaned.columns:\n",
    "    c_outliers = detect_outliers_iqr(df_cleaned, \"Current (A)\")\n",
    "    outlier_indices.update(c_outliers.index)\n",
    "    print(f\"[OUTLIER] Current (A): {len(c_outliers)}\")\n",
    "\n",
    "# Power consumption outliers\n",
    "if \"Power Consumption\" in df_cleaned.columns:\n",
    "    p_outliers = detect_outliers_iqr(df_cleaned, \"Power Consumption\")\n",
    "    outlier_indices.update(p_outliers.index)\n",
    "    print(f\"[OUTLIER] Power Consumption: {len(p_outliers)}\")\n",
    "\n",
    "print(f\"[SUMMARY] TOTAL OUTLIER ROWS: {len(outlier_indices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7c8ee-3f17-4a2f-b3d1-7bdfa22ac99c",
   "metadata": {},
   "source": [
    "### Step 7 – Remove Outliers\n",
    "In this step, all rows identified as outliers in the previous stage are removed from the dataset. \n",
    "This ensures that extreme or unrealistic values do not distort analysis or modeling. \n",
    "The number of removed rows is reported to track data reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1c6ba8-35a1-4928-b1b6-4d18fccdac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 6] REMOVING OUTLIERS\n",
      "================================================================================\n",
      "[INFO] Removed 16 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. REMOVE OUTLIERS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 6] REMOVING OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "before = len(df_cleaned)\n",
    "df_final = df_cleaned.drop(index=outlier_indices)\n",
    "after = len(df_final)\n",
    "\n",
    "print(f\"[INFO] Removed {before - after} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4c8e0-d360-4f84-bbc5-8daa96c9b7ca",
   "metadata": {},
   "source": [
    "### Step 8 – Clean DecommissionStatus\n",
    "This step standardizes the mixed values in the `DecommissionStatus` column by extracting two clean fields: \n",
    "a categorical `OperationalStatus` indicating whether the sensor is active or decommissioned, and a `DecommissionDate` parsed from valid date entries. \n",
    "The original mixed-format column is then removed to improve data consistency and usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b15232-1d29-454b-ad84-6f96433389a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 7] CLEANING 'DecommissionStatus'\n",
      "================================================================================\n",
      "[SUCCESS] Cleaned DecommissionStatus\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. CLEAN DecommissionStatus (Mixed Types)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 7] CLEANING 'DecommissionStatus'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if \"DecommissionStatus\" in df_final.columns:\n",
    "\n",
    "    # Create new categorical status\n",
    "    df_final[\"OperationalStatus\"] = df_final[\"DecommissionStatus\"].apply(\n",
    "        lambda x: \"Operational\" if x == \"Operational\" else \"Decommissioned\"\n",
    "    )\n",
    "\n",
    "    # Extract real dates\n",
    "    df_final[\"DecommissionDate\"] = pd.to_datetime(\n",
    "        df_final[\"DecommissionStatus\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Drop original column\n",
    "    df_final.drop(columns=[\"DecommissionStatus\"], inplace=True)\n",
    "\n",
    "    print(\"[SUCCESS] Cleaned DecommissionStatus\")\n",
    "\n",
    "else:\n",
    "    print(\"[WARN] Column 'DecommissionStatus' missing; skipping cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f071f-e1ed-42eb-a2ee-615312db6d81",
   "metadata": {},
   "source": [
    "### Step 9 – Date Cleaning\n",
    "In this step, all date-related columns are converted into proper `datetime` format to ensure consistent handling of timestamps. \n",
    "Invalid or non-date values are safely converted to `NaT` using `errors='coerce'`, enabling accurate time-based analysis in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988654ea-23f3-410e-b446-45f02d98619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 8] DATE CLEANING\n",
      "================================================================================\n",
      "[INFO] Converted Timestamp to datetime\n",
      "[INFO] Converted InstallationDate to datetime\n",
      "[INFO] Converted DecommissionDate to datetime\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. DATE CLEANING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 8] DATE CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "date_columns = [\"Timestamp\", \"InstallationDate\", \"DecommissionDate\"]\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df_final.columns:\n",
    "        df_final[col] = pd.to_datetime(df_final[col], errors='coerce')\n",
    "        print(f\"[INFO] Converted {col} to datetime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb23e4-1c85-4d64-95f5-acff92c10282",
   "metadata": {},
   "source": [
    "### Step 10 – Drop Non-Informative Columns\n",
    "This step removes columns that do not contribute meaningful information to the analysis. \n",
    "Since the `region` column contains only a single category, it provides no variability and is dropped to simplify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdf4cd-6ff7-4012-99d8-ab58604a801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. DROP USELESS COLUMNS (region)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 9] DROPPING NON-INFORMATIVE COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if \"region\" in df_final.columns:\n",
    "    if df_final[\"region\"].nunique() == 1:\n",
    "        df_final.drop(columns=[\"region\"], inplace=True)\n",
    "        print(\"[INFO] Dropped column 'region' (only one category)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5044c-b23f-43e9-96f7-7d683ed358bd",
   "metadata": {},
   "source": [
    "### Step 11 – Categorical Encoding\n",
    "In this step, categorical features are transformed into numeric format to prepare the dataset for modeling. \n",
    "Selected columns (`Name`, `City`, `Manufacturer`, `Sensor_ID`) are one-hot encoded, while `OperationalStatus` is converted into a binary indicator. \n",
    "This ensures all categorical information is represented in a machine-readable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c799b5-f0ac-4c8f-b5db-650349a2b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 10] CATEGORICAL ENCODING\n",
      "================================================================================\n",
      "[ENCODE] One-hot encoded Name\n",
      "[ENCODE] One-hot encoded City\n",
      "[ENCODE] One-hot encoded Manufacturer\n",
      "[ENCODE] One-hot encoded Sensor_ID\n",
      "[ENCODE] Encoded OperationalStatus (Operational=1, Decommissioned=0)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. CATEGORICAL ENCODING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 10] CATEGORICAL ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_cols = [\"Name\", \"City\", \"Manufacturer\", \"Sensor_ID\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_final.columns:\n",
    "        df_final = pd.get_dummies(df_final, columns=[col], drop_first=True)\n",
    "        print(f\"[ENCODE] One-hot encoded {col}\")\n",
    "\n",
    "# Binary encoding for OperationalStatus\n",
    "if \"OperationalStatus\" in df_final.columns:\n",
    "    df_final[\"OperationalStatus\"] = df_final[\"OperationalStatus\"].map({\n",
    "        \"Operational\": 1,\n",
    "        \"Decommissioned\": 0\n",
    "    })\n",
    "    print(\"[ENCODE] Encoded OperationalStatus (Operational=1, Decommissioned=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d916251-b841-4921-a600-3d326f3b2ea8",
   "metadata": {},
   "source": [
    "### Step 12 – Save Cleaned Data and Report\n",
    "In this final step, the fully cleaned dataset is exported as a new CSV file, and a detailed cleaning report is generated summarizing all transformations. \n",
    "The report includes row counts, removed duplicates and outliers, processed columns, and file locations, ensuring full transparency and reproducibility of the cleaning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bed186d-0d1d-4d9a-b5e8-0c178216cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STEP 11] SAVING RESULTS\n",
      "================================================================================\n",
      "[SUCCESS] Cleaned data saved: cleaned_datasets\\cleaned_data_20251205_130529.csv\n",
      "[SUCCESS] Report saved: cleaned_datasets\\cleaning_report_20251205_130529.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 12. SAVE CLEANED DATA + REPORT\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[STEP 11] SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cleaned_file = os.path.join(OUTPUT_FOLDER, f\"cleaned_data_{timestamp}.csv\")\n",
    "report_file = os.path.join(OUTPUT_FOLDER, f\"cleaning_report_{timestamp}.txt\")\n",
    "\n",
    "df_final.to_csv(cleaned_file, index=False)\n",
    "print(f\"[SUCCESS] Cleaned data saved: {cleaned_file}\")\n",
    "\n",
    "report_text = f\"\"\"\n",
    "CLEANING REPORT – ASIAN POWER GRID\n",
    "===============================================\n",
    "Timestamp: {timestamp}\n",
    "\n",
    "Original rows:     {len(df_original)}\n",
    "Final rows:        {len(df_final)}\n",
    "Duplicates removed: {duplicate_count}\n",
    "Outliers removed:   {before - after}\n",
    "\n",
    "Processed columns:\n",
    "- DecommissionStatus cleaned into OperationalStatus + DecommissionDate\n",
    "- Dates converted to datetime\n",
    "- region column dropped\n",
    "- One-hot encoding for categorical columns\n",
    "- Numeric missing values filled\n",
    "\n",
    "Saved files:\n",
    "- Cleaned CSV: {cleaned_file}\n",
    "- Report:      {report_file}\n",
    "\"\"\"\n",
    "\n",
    "with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"[SUCCESS] Report saved: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3b5691-1a3e-41e9-aa51-4cfcff8b53f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANING PIPELINE COMPLETED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DONE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de6d8e-8cbd-4c04-9120-fa556207233e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
